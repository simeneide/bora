\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{Hierarchical Large Language Models for Multi-task problems}
\author{simen }
\date{November 2023}

\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\usepackage{amsmath}
\addbibresource{references.bib}

\begin{document}

\maketitle

\section{Introduction}

\section{Related works}
[This might be background?]
\begin{itemize}
    \item explain what pretraining LLM means
    \item Explain how LORA fine tuned is done on each task
    \item Explain how LORA finetuned also could be done on all tasks together if they have a similar data distribution.
\end{itemize}


\section{Method: Hierarchical LLM}

\subsection{Task and Data Structure}
We define our study across $D$ distinct tasks, denoted as $d=1,2,...,D$. For each task, our dataset comprises $N_d$ documents, with each document containing a sequence of $W_n$ tokens. This dataset structure is pivotal for our analysis and can be formally represented as:

\begin{equation} \label{eq:data}
\mathcal{D} = \{ \{ w_{d,n,1:W_n} \}_{n=1}^{N_d} \}_{d=1}^D
\end{equation}

In this notation, $w_{d,n,1:W_n}$ represents the token sequence in the $n^{th}$ document of the $d^{th}$ task. For simplification in subsequent discussions, we will omit the indices $d$ and $n$ when their reference is evident from the context.

\subsection{Pretrained Autoregressive Language Model}
Our foundational model is a pretrained autoregressive language model, denoted by its parameters $\theta_{full}$ and its architecture, referred to as $LLM$. The model's probability distribution is given by:

\begin{equation} \label{eq:LLMprob}
P(w_{1:W} | \theta_{full}) = \prod_{i=1}^{W-1} LLM(w_{i+1} | w_{1:i})
\end{equation}
%
Here, $\theta_{full}$ represents the set of parameters obtained from pretraining on a large and diverse dataset, distinct from our current dataset $\mathcal{D}$.
We omit to write $\theta_{full}$ explicitly in the model, as it always depdends on it.

\subsection{The Hierarchical LORA finetuned}
For each task $d$ we have a LORA parameter set $\theta_d$ that is used to fine-tune to that task.
Hence, the likelihood of our problem follows from combining equation \ref{eq:data} and \ref{eq:LLMprob}:
\begin{equation} \label{eq:lik}
    \text{L}(\mathcal{D} | \theta_1, \theta_2, \ldots, \theta_D) = \prod_{d=1}^D \prod_{n=1}^{N_d} \prod_{i=1}^{W_n-1} \text{LLM}(w_{d,n,i+1} | w_{d,n,1:i}; \theta_d)
\end{equation}
%

The prior of 

\begin{equation}
    P(\theta_{1:D}) = \prod_{d=1}^D N(\theta_d ; \Theta, \sigma I)
\end{equation}
%
where $\Theta$ is a set of hierarchical parameters, $I$ is a unit diagonal matrix and $\sigma \leq 0$ is a scalar hyperparameter setting how similar task parameters $\theta_d$ should be the hierarchical parameters $\Theta$.

\begin{itemize}
    \item Typically one finetune each task separately, as written in \ref{eq:lik}.
    \item If the D tasks are more similar to eachother than the pretraining corpus, the optimal parameters $\hat{\theta}_d$ should share some information during training in order to maximize their likelihood.
\end{itemize}


\section{Experimental setup}
We choose the `opt-350m` base model and construct three tasks that it has likely not seem as they are in Norwegian:
\begin{itemize}
    \item A subset of Norwegian parliament speeches of politicians containing 1000 examples
    \item A subset of a translation dataset from Norwegian bokmål to Norwegian Nynorsk (NbAiLab-nbnn-translation) containing 1000 examples
    \item A subset of a translation dataset from Norwegian bokmål to Norwegian Nynorsk (NbAiLab-nbnn-translation) containing 200 examples
\end{itemize}

These datasets will have a shared domain (Norwegian) that is different from what the pretrained foundational model is trained on.
We use two tasks containing the same dataset distribution to see how sharing knowledge through the hierarchical prior can benefit datasets with few examples.


\section{Conclusion}





\section*{Background}
Low-Rank  Adaption (LoRA) of an existing large-scale pre-trained Large Language Models (LLM) \cite{hu_lora_2021} has become a popular go-to approach when solving a specific text problem.

LoRA can reduce the number of trainable parameters by 10'000 times and the GPU memory requirement by 3 times \cite{hu_lora_2021}, while still outperform other fine-tuning techniques.
We are interested in LoRA adaptions for multi-task problems, such as the TULU-v2-mix \cite{ivison_camels_2023}.

For multi-task problems there are two conventional ways of training them. You can either 
\begin{enumerate}
    \item finetune each task separately
    \item train one model for all tasks (given that the textual problem contains which task is being solved for each case)
\end{enumerate}

In addition to these two approaches, we present generalization of the two approaches: the Hierarchical Low-Rank Adaption.
Given a pretrained LLM and $n$ tasks, we construct $n$ sets of LoRA parameters $\theta_n$ that each are trained on its separate task, like in approach (1) above. In additon, we create a global parameter set $\Theta$ that works as a prior to all the task specific parameters (or be regularized towards in a frequentist vocabulary).
The final posterior will look something like this:
%
\begin{equation} \label{eq:posterior}
    \sum_{n=1}^N  \sum_{i \in D_n} P(text_i | \theta_n) + \sum_{n=1}^N N(\theta_n | \Theta, \sigma)
\end{equation}
%
where
$D_n$ is the dataset corresponding to task $n$,
$P(text_i | \theta)$ is the likelihood of sentence $text_i$ evaluated on the parameters $\theta$,

$\sigma$ is a tuning parameter for how close the task specific parameters should be to the global parameter.

\paragraph{Why Hierarchical LLM?}
The Hierarchical LoRA Language Model will allow tasks to share information between each other through the global parameter set $\Theta$.
If a task dataset has little data, the posterior  in equation (\ref{eq:posterior}) will mainly focus on the prior, whereas if the task dataset has a lot of data, the parameters can learn more about the specific task and move away from the global "average" model.

The model above can be seen as a generalisation of the two approaches above.
As $\sigma$ goes towards zero the problem will be equal to approach (2) (assuming we have added the task in the text), 
and as $\sigma$ goes to infinity we get $n$ independent sets of parameters optimized independently as in approach (1).


\paragraph{Research Questions}
We have the following research questions
\begin{itemize}
    \item Will a maximum a posteriori hierarchical LoRA LLM  with one adapter per task outperform a single LoRA with the equivalent number of trainable parameters?
    \item Privacy: Will training a Hierarchical LoRA on multiple tasks be more privacy friendly than training on two tasks jointly?
    \item Will a posterior distribution of a LoRA LLM have "better" diversity/exploration than the sampling techniques currently used (p-sampling, nucleus samplng)
\end{itemize}

\paragraph{Evaluation}
We want to evaluate our research questions by (1) evaluating it on an open benchmark (TULU-v2-mix \cite{ivison_camels_2023}), (2) an evaluation by an GPT-4 agent, and (3) an online evaluation as a journalistic assistant in a news media article CMS, suggesting article attributes such as titles and tags.



\printbibliography

\end{document}
