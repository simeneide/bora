\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{HLoRA: Hierarchical Large Language Models for Multi-task problems}
\author{simen }
\date{November 2023}

\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\R}{\mathbb{R}}
\usepackage{booktabs}
\addbibresource{My Library.bib}

\begin{document}

\maketitle

\section{Introduction}
Large Language Models (LLM) have become a popular tool for solving a wide range of text problems. LLMs are typically pretrained on a large and diverse dataset, and then fine-tuned on a specific task. The fine-tuning process is often done by minimizing the negative log-likelihood of the task-specific dataset, and the resulting model is then used to make predictions on new data. A popular fine-tuning method is Low-Rank Adaption (LoRA), which reduces the number of trainable parameters by 10'000 times and the GPU memory requirement by 3 times, while still outperforming other fine-tuning techniques (\cite{hu_lora_2021}).

This paper presents a generalization of the LoRA fine-tuning method for multi-task problems, which we refer to as Hierarchical LoRA. The Hierarchical LoRA method allows tasks to share information between each other through a global parameter set, and can be seen as a generalization of the two conventional approaches for multi-task problems: (1) finetuning each task separately, and (2) training one model for all tasks. We evaluate the Hierarchical LoRA method on a benchmark dataset, and show that there exist a trade-off between the global and task-specific parameters that can be tuned to optimize the model's performance.

\begin{itemize}
    \item Typically one finetune each task separately, as written in \ref{eq:lik}.
    \item If the D tasks are more similar to eachother than the pretraining corpus, the optimal parameters $\hat{\theta}_d$ should share some information during training in order to maximize their likelihood.
\end{itemize}

\section{Related works}

\subsection{Multi-task Learning}
Multi-task Learning \cite{caruana_multitask_nodate} is a popular machine learning approach for training one model on multiple tasks. A common approach is to share the lower layers of a neural network between tasks, and have separate output layers for each task. Our paper focuses on a different approach, where we share information between tasks through a hierarchical prior over the task-specific parameters.

\subsection{Finetuning methods}
A popular approach to finetuning neural networks involves adjusting only the top layer (or the n top layers) of a pre-trained neural network (e.g. \cite{yosinski2014transferable}). These techniques observe that features learned in the lower layers of the network are general and transferable, while the top layers are more task-specific. This approach has been widely adopted in neural networks. However, we focus on the LoRA method, which allows us to train a low rank decomposition of the entire network.

There are several variations of the LoRA algorithm. The Sparse Mixture of Low Rank Adapters (SIRA) \cite{zhu_sira_2023}, for instance, creates a Mixture of Expert on the low-rank parameters. Another variation, the Weight Decomposed Low-Rank Adaption (DoRA) \cite{hayou_lora_2024}, decomposes the low-rank parameters into scale and unit norm direction parameters. There's also the Efficient Low Rank Adaption (LoRA+) \cite{hayou_lora_2024}, which improves the existing LoRA optimization algorithm by adjusting the learning rate of the low-rank parameters.
All these methods are designed to improve the performance of the LoRA algorithm, and can be used in conjunction with the Hierarchical LoRA method.

\section{Method: Hierarchical LLM}
This section describes the Hierarchical LoRA method for multi-task problems. We start by defining the task and data structure, and then introduce the pretrained autoregressive language model. We then describe the LoRA method, and finally present our Hierarchical LoRA model.

\subsection{Task and Data Structure}
We define our study across $D$ distinct tasks, denoted as $d=1,2,...,D$. For each task $d$, we have a dataset $\mathcal{D}_d$ containing $N_d$ documents, with each document consisting of a sequence of $W_n$ tokens. This dataset structure can be formally represented as:

\begin{equation} \label{eq:data}
\mathcal{D} = \{ \mathcal{D}_d \}_{d=1}^D  =  \{ \{ w_{d,n,1:W_n} \}_{n=1}^{N_d} \}_{d=1}^D
\end{equation}
%
In this notation, $w_{d,n,1:W_n}$ represents the token sequence in the $n^{th}$ document of the $d^{th}$ task. For simplification in subsequent discussions, we will omit the indices $d$ and $n$ when their reference is evident from the context.

\subsection{Pretrained Autoregressive Language Model}
Our foundational model is a pretrained autoregressive language model, denoted by its parameters $\theta_{full}$ and its architecture, referred to as a Large Language Model ($LLM$). The parameters $\theta_{full}$ has been obtained from pretraining on a large and diverse dataset, distinct from our current dataset $\mathcal{D}$. The model's probability distribution is given by:
The model's probability distribution is given by:

\begin{equation} \label{eq:LLMprob}
P(w_{1:W} | \theta_{full}) = \prod_{i=1}^{W-1} LLM(w_{i+1} | w_{1:i})
\end{equation}
%
where $LLM(w_{i+1} | w_{1:i})$ is the probability of the next token $w_{i+1}$ given the previous tokens $w_{1:i}$ in the document.
We omit to write $\theta_{full}$ explicitly in the model, as the autoregressive language model always depend on these parameters.

\subsection{Low-Rank Adaption (LoRA)}
A Large Language Model contains many linear layers, and the LoRA method reduces the number of trainable parameters by reparameterizing the weight parameters ($W \in \R^{n_1 \times n_2}$) of these layers into the original pretrained weights $W_{full}$ and a low-rank decomposition $A$ and $B$ (\cite{hayou_lora_2024}):
\begin{equation} \label{eq:LoRA}
    W = W_{full} + \frac{\alpha}{r} BA 
\end{equation}
%
where $W_{full} \in \R^{n_1 \times n_2}$ is the original pretrained weight, $B \in \R^{n_1 \times r}$ and $A \in \R^{r \times n_2}$ are the low-rank factors, $r \ll \text{min}(n_1, n_2)$ is the low rank dimension and $\alpha$ is a scaling hyperparameter.

During LoRA fine-tuning, the low-rank factors $A$ and $B$ are learned from data as normal parameter matricies, while the original pretrained weights $W_{full}$ are kept fixed.

\subsection{The Hierarchical LoRA model}
In the hierarchical LoRA model, we introduce a LoRA parameter set for each task $d$, denoted as $\theta_d$, containing all the low-rank decomposition matricies $A$ and $B$ from each decomposed layer in the Large Language Model. The likelihood of the data $\mathcal{D}$ given the task parameters $\theta_{1:D}$ is then given by (combining equation \ref{eq:data} and \ref{eq:LLMprob}):
\begin{equation} \label{eq:lik}
    \text{L}(\mathcal{D} | \theta_1, \theta_2, \ldots, \theta_D) = \prod_{d=1}^D L(\mathcal{D}_d | \theta_d) = \prod_{d=1}^D \prod_{n=1}^{N_d} \prod_{i=1}^{W_n-1} \text{LLM}(w_{d,n,i+1} | w_{d,n,1:i}; \theta_d)
\end{equation}
%
%
Each task will share some information with each other, and we model this by introducing a Gaussian hierarchical prior over the task parameters $\theta_{1:D}$, given by
\begin{equation} \label{eq:prior}
    P(\theta_{1:D} | \Theta, \tau ) = \prod_{d=1}^D N(\theta_d ; \Theta, \frac{1}{\tau} I)
\end{equation}
%
where $\Theta$ is a set of hierarchical mean parameters, $I$ is a unit diagonal matrix and $\tau \geq 0$ is a scalar precision hyperparameter controlling how similar task parameters $\theta_d$ should be to the hierarchical mean parameters $\Theta$.
We let $\Theta$ have a uniform prior (i.e. $P(\Theta) = 1$).

The posterior distribution of the hierarchical model is then given by combining equation \ref{eq:lik} and \ref{eq:prior}:
\begin{equation} \label{eq:posterior}
    P(\theta_{1:D} | \mathcal{D}, \tau) \propto \prod_{d=1}^D \text{L}(\mathcal{D}_d | \theta_d) \cdot P(\theta_d | \Theta, \tau)
\end{equation}
%

\subsection{Optimization}
Given a precision hyperparameter, we want to find the maximum a posteriori (MAP) estimate of the hierarchical model. 
We will do this by using AdamW (\cite{adamW}), a gradient-based optimization algorithm, optimizing over the task parameters $\theta_{1:D}$ and the hierarchical mean parameters $\Theta$ simultaneously optimizing the likelihood and the prior in equation \ref{eq:posterior}.
This implies that the task parameters with have gradients both towards minimizing the convential LLM loss and the hierarchical mean parameters. The hierarchical mean parameters $\Theta$ will have gradients towards the average of the task parameters, and learn to represent the global structure of the tasks after the task parameters have been partly optimized.

An important observation is that the gradient of the prior term in equation \ref{eq:posterior} is proportional to the precision hyperparameter $\tau$. This means that the optimization will more steps to converge for larger values of $\tau$, as the task parameters will be more constrained towards the hierarchical mean parameters.
Therefore, we adjust the learning rate of the AdamW optimizer to be proportional to the precision hyperparameter $\tau$, and we find in informal tests that this cause the optimizations to converge at a similar rate for different values of $\tau$.

\section{Experimental setup}
% Use the Talk of Norway Dataset consisting of speeches of Norwegian parliament politicians. We will consider different speakers of this dataset as different tasks, and see how the hierarchical model can benefit from sharing information between these tasks.

% As base model, use opt-350m as it is predominantly trained on English, and only has seen a small amount of Norwegian data from the commoncrawls dataset.
To test our method, we optimize the model with a range of different precision hyperparameters $\tau$, and evaluate the model's perplexity on the test set for each task.


We use the Talk of Norway Dataset \cite{lapponi_talk_2018}, which consists of speeches of Norwegian parliament politicians. Each speaker will come from different political parties and geographical areas, but the tasks will still share a common domain. Therefore, we can consider different speakers of this dataset as different tasks. Specifically, we select chronologically the 25 first speakers that have above 100 speeches in the dataset.
This gives us a total of 25 tasks with samples ranging from 110 to 477 documents, and an average of 202 documents per task.
The distibution of samples per task can be seen in Figure \ref{fig:task_len}.
We leave 33\% of the data for the test set.

% add figure of the dataset distribution in file training_lengths.png
\begin{figure}[h] \label{fig:task_len}
    \centering
    \includegraphics[width=\textwidth]{figures/training_lengths.png}
    \caption{Number of training examples for each task}
    \label{fig:training_lengths}
\end{figure}

We select a model that has been pretrained on a large and diverse dataset, but has not seen the specific tasks that we want to evaluate it on. We will use the `opt-350m` model \cite{zhang_opt_2022}, which is predominantly trained on English, and has only seen a small amount of Norwegian data from the CommonCrawls dataset.

Adapting learning rate to the precision hyperparameter..

\section{Results}
The results of the hierarchical LoRA model on the Talk of Norway Dataset are shown in Table \ref{tbl:results} and Figure \ref{fig:results_plot}. We see that the model achieves the best validation perplexity of 12.82 when using a learning rate of $10^{-3}$ and a regularization constant of $10^{2}$.
This indicates that the model benefits from sharing information between the tasks, and that the global prior helps to improve the performance of the individual task parameters.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/results_plot.png}
    \caption{Plot of Regularization Constant vs Perplexity}
    \label{fig:results_plot}
\end{figure}

\begin{table}[h]
\centering
\caption{Empirical results of the hierarchical LORA finetuned on the three tasks. The perplexity is calculated on the test set.}
\label{tbl:results}
\begin{tabular}{ccc}
    \toprule
    Learning Rate & Regularization & Validation Perplexity \\
    \midrule
        $10^{-4}$ &              0 &                 16.80 \\
        $10^{-5}$ &       $10^{0}$ &                 16.59 \\
        $10^{-4}$ &       $10^{1}$ &                 12.85 \\
        $10^{-3}$ &       $10^{2}$ &                 \textbf{12.82} \\
        $10^{-2}$ &       $10^{3}$ &                 13.26 \\
        $10^{-1}$ &       $10^{4}$ &                 13.91 \\
    \bottomrule
    \end{tabular}
\end{table}

\section{Discussion}

% Discuss overall result
% Fetch perplexity for each task and discuss how it improves results significantly for some tasks (less data?), and less larger data tasks.
% Give some advice on how to use this method in practice.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/weights_vs_data_lengths.png}
    \caption{The figure shows the L2-distance between each task's adapter weight and the global prior on the y-axis, and the number of training examples on the x-axis. We see that, as expected, tasks with more training data will have a larger distance to the global prior.}
    \label{fig:weights_vs_datalen} 
\end{figure}

%\end{document}


\section{Conclusion}

\section*{Background}
Low-Rank  Adaption (LoRA) of an existing large-scale pre-trained Large Language Models (LLM) \cite{hu_lora_2021} has become a popular go-to approach when solving a specific text problem.

LoRA can reduce the number of trainable parameters by 10'000 times and the GPU memory requirement by 3 times \cite{hu_lora_2021}, while still outperform other fine-tuning techniques.
We are interested in LoRA adaptions for multi-task problems, such as the TULU-v2-mix \cite{ivison_camels_2023}.

For multi-task problems there are two conventional ways of training them. You can either 
\begin{enumerate}
    \item finetune each task separately
    \item train one model for all tasks (given that the textual problem contains which task is being solved for each case)
\end{enumerate}

In addition to these two approaches, we present generalization of the two approaches: the Hierarchical Low-Rank Adaption.
Given a pretrained LLM and $n$ tasks, we construct $n$ sets of LoRA parameters $\theta_n$ that each are trained on its separate task, like in approach (1) above. In additon, we create a global parameter set $\Theta$ that works as a prior to all the task specific parameters (or be regularized towards in a frequentist vocabulary).
The final posterior will look something like this:
%
\begin{equation} \label{eq:posterior}
    \sum_{n=1}^N  \sum_{i \in D_n} P(text_i | \theta_n) + \sum_{n=1}^N N(\theta_n | \Theta, \sigma)
\end{equation}
%
where
$D_n$ is the dataset corresponding to task $n$,
$P(text_i | \theta)$ is the likelihood of sentence $text_i$ evaluated on the parameters $\theta$,

$\sigma$ is a tuning parameter for how close the task specific parameters should be to the global parameter.

\paragraph{Why Hierarchical LLM?}
The Hierarchical LoRA Language Model will allow tasks to share information between each other through the global parameter set $\Theta$.
If a task dataset has little data, the posterior  in equation (\ref{eq:posterior}) will mainly focus on the prior, whereas if the task dataset has a lot of data, the parameters can learn more about the specific task and move away from the global "average" model.

The model above can be seen as a generalisation of the two approaches above.
As $\sigma$ goes towards zero the problem will be equal to approach (2) (assuming we have added the task in the text), 
and as $\sigma$ goes to infinity we get $n$ independent sets of parameters optimized independently as in approach (1).


\paragraph{Research Questions}
We have the following research questions
\begin{itemize}
    \item Will a maximum a posteriori hierarchical LoRA LLM  with one adapter per task outperform a single LoRA with the equivalent number of trainable parameters?
    \item Privacy: Will training a Hierarchical LoRA on multiple tasks be more privacy friendly than training on two tasks jointly?
    \item Will a posterior distribution of a LoRA LLM have "better" diversity/exploration than the sampling techniques currently used (p-sampling, nucleus samplng)
\end{itemize}

\paragraph{Evaluation}
We want to evaluate our research questions by (1) evaluating it on an open benchmark (TULU-v2-mix \cite{ivison_camels_2023}), (2) an evaluation by an GPT-4 agent, and (3) an online evaluation as a journalistic assistant in a news media article CMS, suggesting article attributes such as titles and tags.



\printbibliography

\end{document}
