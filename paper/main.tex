\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{Hierarchical Large Language Models for Multi-task problems}
\author{simen }
\date{November 2023}

\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\usepackage{amsmath}
\addbibresource{references.bib}

\begin{document}

\maketitle

\section*{Background}
Low-Rank  Adaption (LoRA) of an existing large-scale pre-trained Large Language Models (LLM) \cite{hu_lora_2021} has become a popular go-to approach when solving a specific text problem.

LoRA can reduce the number of trainable parameters by 10'000 times and the GPU memory requirement by 3 times \cite{hu_lora_2021}, while still outperform other fine-tuning techniques.
We are interested in LoRA adaptions for multi-task problems, such as the TULU-v2-mix \cite{ivison_camels_2023}.

For multi-task problems there are two conventional ways of training them. You can either 
\begin{enumerate}
    \item finetune each task separately
    \item train one model for all tasks (given that the textual problem contains which task is being solved for each case)
\end{enumerate}

In addition to these two approaches, we present generalization of the two approaches: the Hierarchical Low-Rank Adaption.
Given a pretrained LLM and $n$ tasks, we construct $n$ sets of LoRA parameters $\theta_n$ that each are trained on its separate task, like in approach (1) above. In additon, we create a global parameter set $\Theta$ that works as a prior to all the task specific parameters (or be regularized towards in a frequentist vocabulary).
The final posterior will look something like this:
%
\begin{equation} \label{eq:posterior}
    \sum_{n=1}^N  \sum_{i \in D_n} P(text_i | \theta_n) + \sum_{n=1}^N N(\theta_n | \Theta, \sigma)
\end{equation}

where
$D_n$ is the dataset corresponding to task $n$,
$P(text_i | \theta)$ is the likelihood of sentence $text_i$ evaluated on the parameters $\theta$,

$\sigma$ is a tuning parameter for how close the task specific parameters should be to the global parameter.

\paragraph{Why Hierarchical LLM?}
The Hierarchical LoRA Language Model will allow tasks to share information between each other through the global parameter set $\Theta$.
If a task dataset has little data, the posterior  in equation (\ref{eq:posterior}) will mainly focus on the prior, whereas if the task dataset has a lot of data, the parameters can learn more about the specific task and move away from the global "average" model.

The model above can be seen as a generalisation of the two approaches above.
As $\sigma$ goes towards zero the problem will be equal to approach (2) (assuming we have added the task in the text), 
and as $\sigma$ goes to infinity we get $n$ independent sets of parameters optimized independently as in approach (1).


\paragraph{Research Questions}
We have the following research questions
\begin{itemize}
    \item Will a maximum a posteriori hierarchical LoRA LLM  with one adapter per task outperform a single LoRA with the equivalent number of trainable parameters?
    \item Privacy: Will training a Hierarchical LoRA on multiple tasks be more privacy friendly than training on two tasks jointly?
    \item Will a posterior distribution of a LoRA LLM have "better" diversity/exploration than the sampling techniques currently used (p-sampling, nucleus samplng)
\end{itemize}

\paragraph{Evaluation}
We want to evaluate our research questions by (1) evaluating it on an open benchmark (TULU-v2-mix \cite{ivison_camels_2023}), (2) an evaluation by an GPT-4 agent, and (3) an online evaluation as a journalistic assistant in a news media article CMS, suggesting article attributes such as titles and tags.


\section{Introduction}
\section{Related works}
\section{Method: Hierarchical LLM}
\section{Experiments}
\section{Conclusion}


\printbibliography

\end{document}
